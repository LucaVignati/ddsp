{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a DDSP Autoencoder on GPU\n",
    "\n",
    "This notebook demonstrates how to install the DDSP library and train it for synthesis based on your own data using our command-line scripts. If run inside of Colab, it will automatically use a free Google Cloud GPU.\n",
    "\n",
    "At the end, you'll have a custom-trained checkpoint that you can download to use with the [DDSP Timbre Transfer Colab](https://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/demos/timbre_transfer.ipynb).\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/ddsp/additive_diagram/ddsp_autoencoder.png\" alt=\"DDSP Autoencoder figure\" width=\"700\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make directories to save model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_DIR = Path('/home/luca/Development')\n",
    "\n",
    "DATA_DIR = PROJECT_DIR.joinpath('data_l2_fixed_3')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATASET_DIR = PROJECT_DIR.joinpath('dataset')\n",
    "\n",
    "DATASET = 'chitarra_michele_rossi/unite'\n",
    "\n",
    "AUDIO_DIR = DATASET_DIR.joinpath(DATASET)\n",
    "\n",
    "TFRECORDS_DIR = PROJECT_DIR.joinpath('TFRecords').joinpath(DATASET)\n",
    "TFRECORDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_TFRECORD_FILEPATTERN = TFRECORDS_DIR.joinpath('train.tfrecord-train*')\n",
    "TEST_TFRECORD_FILEPATTERN = TFRECORDS_DIR.joinpath('train.tfrecord-eval*')\n",
    "os.environ['LD_LIBRARY_PATH'] = os.environ['CONDA_PREFIX'] + '/lib'\n",
    "# os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=' + os.environ['CONDA_PREFIX'] + '/lib'\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/home/luca/miniconda3/envs/tf'\n",
    "print(os.environ)\n",
    "!printenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable memory growth\n",
    "This prevents tensorflow to allocate all the available memory as soon as it starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "\n",
    "  try:\n",
    "    for gpu in physical_devices:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "      # Memory growth must be set before GPUs have been initialized.\n",
    "      print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess raw audio into TFRecord dataset\n",
    "\n",
    "We need to do some preprocessing on the raw audio you uploaded to get it into the correct format for training. This involves turning the full audio into short (4-second) examples, inferring the fundamental frequency (or \"pitch\") with [CREPE](http://github.com/marl/crepe), and computing the loudness. These features will then be stored in a sharded [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) file for easier loading. Depending on the amount of input audio, this process usually takes a few minutes.\n",
    "\n",
    "* (Optional) Transfer dataset from drive. If you've already created a dataset, from a previous run, this cell will skip the dataset creation step and copy the dataset from `$DRIVE_DIR/data` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_files = list(TFRECORDS_DIR.glob('*'))\n",
    "if len(dataset_files) == 0:\n",
    "    if not AUDIO_DIR.glob('*'):\n",
    "        raise ValueError('No audio files found in {}'.format(AUDIO_DIR))\n",
    "\n",
    "    AUDIO_FILEPATTERN = str(AUDIO_DIR.joinpath('*les*'))\n",
    "    TRAIN_TFRECORD = str(TFRECORDS_DIR.joinpath('train.tfrecord'))\n",
    "\n",
    "    !ddsp_prepare_tfrecord \\\n",
    "        --input_audio_filepatterns=\"$AUDIO_FILEPATTERN\" \\\n",
    "        --output_tfrecord_path=\"$TRAIN_TFRECORD\" \\\n",
    "        --num_shards=10 \\\n",
    "        --example_secs=10 \\\n",
    "        --hop_secs=1 \\\n",
    "        --eval_split_fraction=0.2 \\\n",
    "        --alsologtostderr    \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset statistics for timbre transfer\n",
    "\n",
    "Quantile normalization helps match loudness of timbre transfer inputs to the \n",
    "loudness of the dataset, so let's calculate it here and save in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp.local import local_utils\n",
    "import ddsp.training\n",
    "\n",
    "data_provider = ddsp.training.data.TFRecordProvider(str(TRAIN_TFRECORD_FILEPATTERN))\n",
    "dataset = data_provider.get_dataset(shuffle=False)\n",
    "PICKLE_FILE_PATH = DATA_DIR.joinpath('dataset_statistics.pkl')\n",
    "\n",
    "_ = local_utils.save_dataset_statistics(data_provider, PICKLE_FILE_PATH, batch_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset in the `ddsp` library and have a look at one of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp.local import local_utils\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 3\n",
    "sequence_length = 2\n",
    "audio_rate = 32000\n",
    "data_rate = 1000\n",
    "\n",
    "data_provider = ddsp.training.data.TFRecordProvider(str(TRAIN_TFRECORD_FILEPATTERN))\n",
    "# dataset = data_provider.get_dataset(shuffle=False)\n",
    "dataset = data_provider.get_batch(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataset_iter = iter(dataset)\n",
    "\n",
    "audio = np.empty((batch_size,0))\n",
    "loudness = np.empty((batch_size,0))\n",
    "f0 = np.empty((batch_size,0))\n",
    "f0_confidance = np.empty((batch_size,0))\n",
    "for n in range(sequence_length):\n",
    "  try:\n",
    "    ex = next(dataset_iter)\n",
    "    audio = np.concatenate((audio, ex['audio']), axis=-1)\n",
    "    loudness = np.concatenate((loudness, ex['loudness_db']), axis=-1)\n",
    "    f0 = np.concatenate((f0, ex['f0_hz']), axis=-1)\n",
    "    f0_confidance = np.concatenate((f0_confidance, ex['f0_confidence']), axis=-1)\n",
    "  except StopIteration:\n",
    "    raise ValueError(\n",
    "        'TFRecord contains no examples. Please try re-running the pipeline with '\n",
    "        'different audio file(s).')\n",
    "      \n",
    "\n",
    "for n in range(batch_size):\n",
    "  local_utils.specplot(audio[n])\n",
    "  local_utils.play(audio[n])\n",
    "\n",
    "  f, ax = plt.subplots(3, 1, figsize=(14, 4))\n",
    "  x = np.linspace(0, 10.0*sequence_length, 2500*sequence_length)\n",
    "  ax[0].set_ylabel('loudness_db')\n",
    "  ax[0].plot(x, loudness[n])\n",
    "  ax[1].set_ylabel('F0_Hz')\n",
    "  ax[1].set_xlabel('seconds')\n",
    "  ax[1].plot(x, f0[n])\n",
    "  ax[2].set_ylabel('F0_confidence')\n",
    "  ax[2].set_xlabel('seconds')\n",
    "  ax[2].plot(x, f0_confidance[n])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "We will now train a \"solo instrument\" model. This means the model is conditioned only on the fundamental frequency (f0) and loudness with no instrument ID or latent timbre feature. If you uploaded audio of multiple instruemnts, the neural network you train will attempt to model all timbres, but will likely associate certain timbres with different f0 and loudness conditions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start up a [TensorBoard](https://www.tensorflow.org/tensorboard) to monitor our loss as training proceeds. \n",
    "\n",
    "Initially, TensorBoard will report `No dashboards are active for the current data set.`, but once training begins, the dashboards should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# import tensorboard as tb\n",
    "# tb.notebook.start('--logdir \"{}\"'.format(DATA_DIR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now begin training. \n",
    "\n",
    "Note that we specify [gin configuration](https://github.com/google/gin-config) files for the both the model architecture ([solo_instrument.gin](TODO)) and the dataset ([tfrecord.gin](TODO)), which are both predefined in the library. You could also create your own. We then override some of the spefic params for `batch_size` (which is defined in in the model gin file) and the tfrecord path (which is defined in the dataset file). \n",
    "\n",
    "### Training Notes:\n",
    "* Models typically perform well when the loss drops to the range of ~4.5-5.0.\n",
    "* Depending on the dataset this can take anywhere from 5k-30k training steps usually.\n",
    "* The default is set to 30k, but you can stop training at any time, and for timbre transfer, it's best to stop before the loss drops too far below ~5.0 to avoid overfitting.\n",
    "* On the colab GPU, this can take from around 3-20 hours. \n",
    "* We **highly recommend** saving checkpoints directly to your drive account as colab will restart naturally after about 12 hours and you may lose all of your checkpoints.\n",
    "* By default, checkpoints will be saved every 300 steps with a maximum of 10 checkpoints (at ~60MB/checkpoint this is ~600MB). Feel free to adjust these numbers depending on the frequency of saves you would like and space on your drive.\n",
    "* If you're restarting a session and `DRIVE_DIR` points a directory that was previously used for training, training should resume at the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIN_FILE_1 = PROJECT_DIR.joinpath('ddsp', 'ddsp', 'training', 'gin', 'models', 'predictor.gin')\n",
    "GIN_FILE_2 = PROJECT_DIR.joinpath('ddsp', 'ddsp', 'training', 'gin', 'datasets', 'tfrecord_predictor.gin')\n",
    "\n",
    "!ddsp_run \\\n",
    "  --mode=train \\\n",
    "  --alsologtostderr \\\n",
    "  --save_dir=\"$DATA_DIR\" \\\n",
    "  --gin_file=\"$GIN_FILE_1\" \\\n",
    "  --gin_file=\"$GIN_FILE_2\" \\\n",
    "  --gin_param=\"TFRecordProvider.file_pattern=\\\"$TRAIN_TFRECORD_FILEPATTERN\\\"\" \\\n",
    "  --gin_param=\"batch_size=16\" \\\n",
    "  --gin_param=\"train_util.train.num_steps=3000000\" \\\n",
    "  --gin_param=\"train_util.train.steps_per_save=300\" \\\n",
    "  --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ddsp_run \\\n",
    "  --mode=eval \\\n",
    "  --alsologtostderr \\\n",
    "  --save_dir=\"$DATA_DIR\" \\\n",
    "  --gin_file=\"$GIN_FILE_1\" \\\n",
    "  --gin_file=\"$GIN_FILE_2\" \\\n",
    "  --gin_param=\"TFRecordProvider.file_pattern=\\\"$TEST_TFRECORD_FILEPATTERN\\\"\" \\\n",
    "  --run_once \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "from ddsp.training.ddsp_run import get_gin_path\n",
    "import gin\n",
    "from ddsp.training import train_util\n",
    "from ddsp.training import models\n",
    "from ddsp.training import trainers\n",
    "from ddsp.training import cloud\n",
    "import gin\n",
    "\n",
    "gfile = tf.io.gfile\n",
    "\n",
    "GIN_PATH = get_gin_path()\n",
    "\n",
    "GIN_FILE_1 = str(PROJECT_DIR.joinpath('ddsp', 'ddsp', 'training', 'gin', 'models', 'predictor.gin'))\n",
    "GIN_FILE_2 = str(PROJECT_DIR.joinpath('ddsp', 'ddsp', 'training', 'gin', 'datasets', 'tfrecord_predictor.gin'))\n",
    "\n",
    "restore_dir = os.path.expanduser('')\n",
    "save_dir = os.path.expanduser(DATA_DIR)\n",
    "# If no separate restore directory is given, use the save directory.\n",
    "restore_dir = save_dir if not restore_dir else restore_dir\n",
    "logging.info('Restore Dir: %s', restore_dir)\n",
    "logging.info('Save Dir: %s', save_dir)\n",
    "\n",
    "gfile.makedirs(restore_dir)  # Only makes dirs if they don't exist.\n",
    "\n",
    "# Enable parsing gin files on Google Cloud.\n",
    "gin.config.register_file_reader(tf.io.gfile.GFile, tf.io.gfile.exists)\n",
    "# Add user folders to the gin search path.\n",
    "for gin_search_path in [GIN_PATH] + []:\n",
    "    gin.add_config_file_search_path(gin_search_path)\n",
    "\n",
    "# Parse gin configs, later calls override earlier ones.\n",
    "with gin.unlock_config():\n",
    "    # Optimization defaults.\n",
    "    use_tpu = bool('')\n",
    "    opt_default = 'base.gin' if not use_tpu else 'base_tpu.gin'\n",
    "    gin.parse_config_file(os.path.join('optimization', opt_default))\n",
    "    eval_default = 'eval/basic.gin'\n",
    "    gin.parse_config_file(eval_default)\n",
    "\n",
    "    # Load operative_config if it exists (model has already trained).\n",
    "    try:\n",
    "        operative_config = train_util.get_latest_operative_config(restore_dir)\n",
    "        logging.info('Using operative config: %s', operative_config)\n",
    "        operative_config = cloud.make_file_paths_local(operative_config, GIN_PATH)\n",
    "        gin.parse_config_file(operative_config, skip_unknown=True)\n",
    "    except FileNotFoundError:\n",
    "        logging.info('Operative config not found in %s', restore_dir)\n",
    "\n",
    "    # User gin config and user hyperparameters from flags.\n",
    "    gin_params = [\n",
    "        'TFRecordProvider.file_pattern=\"' + str(TRAIN_TFRECORD_FILEPATTERN) + '\"',\n",
    "        'batch_size=16',\n",
    "        'train_util.train.num_steps=3000000',\n",
    "        'train_util.train.steps_per_save=300',\n",
    "        'trainers.Trainer.checkpoints_to_keep=10'\n",
    "    ]\n",
    "    gin_file = cloud.make_file_paths_local([GIN_FILE_1, GIN_FILE_2], GIN_PATH)\n",
    "    gin.parse_config_files_and_bindings(\n",
    "        gin_file, gin_params, skip_unknown=True)\n",
    "\n",
    "logging.info('Operative Gin Config:\\n%s', gin.config.config_str())\n",
    "\n",
    "# Training.\n",
    "strategy = train_util.get_strategy('',\n",
    "                                    cluster_config='')\n",
    "with strategy.scope():\n",
    "    model = models.get_model()\n",
    "    trainer = trainers.get_trainer_class()(model, strategy)\n",
    "\n",
    "train_util.train(data_provider=gin.REQUIRED,\n",
    "                    trainer=trainer,\n",
    "                    save_dir=save_dir,\n",
    "                    restore_dir=restore_dir,\n",
    "                    early_stop_loss_value=None,\n",
    "                    report_loss_to_hypertune=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resynthesis\n",
    "\n",
    "Check how well the model reconstructs the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp.local.local_utils import play, specplot\n",
    "import ddsp.training\n",
    "import gin\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from ddsp.training.preprocessing import scale_f0_hz, scale_db\n",
    "\n",
    "TEST_TFRECORD_FILEPATTERN = str(PROJECT_DIR.joinpath('TFRecords', 'chitarra_michele_rossi', 'separate', '9_les_neck_pick', 'train.tfrecord*'))\n",
    "\n",
    "data_provider = ddsp.training.data.TFRecordProvider(str(TRAIN_TFRECORD_FILEPATTERN), example_secs=10)\n",
    "dataset = data_provider.get_batch(batch_size=30, shuffle=True)\n",
    "\n",
    "try:\n",
    "  batch = next(iter(dataset))\n",
    "except OutOfRangeError:\n",
    "  raise ValueError(\n",
    "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
    "      'different audio file(s).')\n",
    "\n",
    "# Parse the gin config.\n",
    "# gin_file = DATA_DIR.joinpath('operative_config-0.gin')\n",
    "gin_file = str(PROJECT_DIR.joinpath('ddsp', 'ddsp', 'training', 'gin', 'models', 'predictor.gin'))\n",
    "gin.parse_config_file(gin_file)\n",
    "\n",
    "# Load model\n",
    "model = ddsp.training.models.Predictor()\n",
    "model.restore(DATA_DIR)\n",
    "\n",
    "# print(batch)\n",
    "\n",
    "# Resynthesize audio.\n",
    "before = datetime.now()\n",
    "outputs = model(batch, training=False)\n",
    "# print(outputs)\n",
    "after = datetime.now()\n",
    "# audio_gen = model.get_audio_from_outputs(outputs)\n",
    "# audio = batch['audio']\n",
    "\n",
    "time = after - before\n",
    "print(time)\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "f, ax = plt.subplots(2, 1, figsize=(14, 4))\n",
    "x = np.linspace(0, 4.0, 2499)\n",
    "ax[0].set_ylabel('loudness_db')\n",
    "ax[0].plot(x, scale_db(batch['loudness_db'][0, :-1]))\n",
    "ax[0].plot(x, outputs['ld_scaled'][0, 1:])\n",
    "ax[1].set_ylabel('F0_Hz')\n",
    "ax[1].set_xlabel('seconds')\n",
    "ax[1].plot(x, np.squeeze(scale_f0_hz(batch['f0_hz'][0, :-1])))\n",
    "ax[1].plot(x, np.squeeze(outputs['f0_scaled'][0, 1:]))\n",
    "# Legend: red = original, blue = resynthesis\n",
    "ax[0].legend(['original', 'prediction'], loc='upper right')\n",
    "\n",
    "# print('Original Audio')\n",
    "# specplot(audio)\n",
    "# play(audio)\n",
    "\n",
    "# print('Resynthesis')\n",
    "# specplot(audio_gen)\n",
    "# play(audio_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc869e07a7caaee0a5175e3600940cd211c6ade1c18c44b4aabe0b0eaa1970e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
